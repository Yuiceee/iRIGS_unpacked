---
title: "WorkingNotebook1"
author: "Colleen Xu"
date: "1/14/2020-2/12/2020"
output: 
  html_document:
      code_folding: hide  # there is a toggle to allow reader to see code for code blocks with include=TRUE
      toc: true
      toc_float: true
      toc_depth: 3
      theme: readable
---

```{r setup, results='hide'}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)

library(tidyverse)
library(skimr)  ## skim function for looking at variable/table statistics
library(here)  ## set up for file paths in machine-agnostic way (Linux/Macs or Windows/PC)
library(metap)
## note that the library metap() can be used to combine P-values by Fisher's method and is used to check results in a code chunk of that section
## See documentation at https://cran.r-project.org/web/packages/metap/
## I had to use Bioconductor to download a dependency: the package 'multtest'. 

here()  ## set "home" location for relative paths. 
```

## Intro

This is an R-notebook re-implementation of iRIGS (Integrative framework for Risk Gene Selector). The [original R code](https://www.vumc.org/cgg/irigs) was released by the Computational Genetics and Genomics Lab at Vanderbilt University as part of the [*Nature Methods* paper](https://www.ncbi.nlm.nih.gov/pubmed/30988527):  

Wang Q, Chen R, Cheng F, Wei Q, Ji Y, Yang H, et al. A Bayesian framework that integrates multi-omics data and gene networks predicts risk genes from schizophrenia GWAS data. Nature Neuroscience. 2019 May;22(5):691â€“9.  

I relied heavily on the methods of this paper and its [Supplementary Note](https://www.nature.com/articles/s41593-019-0382-7#Sec26).  

**My implementation differs from the original code in several ways. All major changes are noted and explained in detail throughout the notebook. A brief description of changes:**   

* Using tidyverse, matrix/dataframe objects, and here packages  
* Preprocessing the input gene information differently, before picking genes from flanking regions around GWAS SNPs   
* Treating missing values for numbers of enhancers the same (replaced with zeroes)
* reproducing the *Nature Methods* paper's Supplementary Table 1 as output  

### Description of iRIGS

This framework was designed to find the genes "most likely" to be disease risk genes. Starting with schizophrenia (SCZ) GWAS data, the genes neighboring the SNPs (loci) are found. During Gibbs sampling, the genes for a loci are weighed based on:  

* their "closeness" to the other sampled genes (in other loci) in a gene-gene network made by the authors   
  - with the code, the iRIGS authors provided the transition probability matrix obtained by running the random walk with restart (RWR) algorithm on their original network. Their original network was dense, with edge weights between genes based on shared GO annotations. {?? how was this made? Is this the transition probabilities from the RWR or just the normalized weighted adjacency matrix for the original network? }  
* their genomic distance from the SNP (Transcription start site (TSS) to SNP in genomic coordinates): abbreviated DTS (distance from transcription start) 
  - based on gene information file provided by iRIGS authors {?? Is where did this come from / when downloaded?}  
* the number of enhancers (predicted distal regulatory element (DRE) - promoter links) they have  
  - Brain HiC data from [the developing cerebral cortex - cortical/subcortical plate (CP) and germinal zone (GZ), Supplementary Tables 23 and 24](https://www.nature.com/articles/nature19847#Sec31): tsv versions provided by iRIGs authors     
  - CapHiC data from the cell line GM12878 (not specific to brain): processed data file provided by iRIGs authors        
  - FANTOM5 data: data file provided by iRIGs authors {?? old version?}    
* de novo mutation (DNM) data for schizophrenia from literature: processed data files provided by iRIGS authors   
* differential expression (DE) data comparing schizophrenia vs control cases from the Commonmind consortium: data file provided by iRIGS authors     

Notes:  
* It is assumed that a gene with more "incoming regulatory links" (enhancers) is more likely to be a disease gene (discussed in the original paper).  
* All genomic coordinates are in hg19 (GRCh37).  

## Setting arguments
**Changed from original paper to save the original results file and a table similar to iRIGS authors' Supplementary Table 1.** Arguments in order, 5 total:  

* SNP_file: path of file with GWAS SNPs. The file must include three columns named (case matters): SNP, Chr, and Pos_hg19   
* flank: integer (in bp, recommended by iRIGS authors: 1000000 for a 2MB flanking region total). The flanking region is the region *flank* base-pairs upstream and *flank* base-pairs downstream of a GWAS SNP. In other words, *flank* is 1/2 the length of the flanking region. Genes in the flanking region will be used as candidate genes during Gibbs sampling    
* path for result files directory   
* path for original type of result file    
* path for result file of the same format as Supplementary Table 1 from the paper   

**Remember to change these last two parameters between runs if you don't want to rewrite your files.**   

Below, I assume that the user of this notebook has downloaded the data from https://www.vumc.org/cgg/irigs and the notebook is in the same level/directory as the supporting_files directory and SNP_file directory.  
```{r arguments}
arguments <- c(here("SNP_file", "SCZ_108_loci"), 
               1000000, 
               here("iRIGS_results"), 
               here("iRIGS_results", "SCZ_trial1_original.txt"), 
               here("iRIGS_results", "SCZ_trial1_supple1.txt"))
```

## Find candidate genes for each SNP 

**CHANGES from original code:** 
* I am filtering the human gene list BEFORE finding candidate genes for SNPs; this should make the search for candidate genes more efficient.  
* I am filtering differently: original code relies heavily on the "official_name" column, but I will be working with ENSG IDs ("Name" column) and the "alias_symbol" column instead. There is an EDA code chunk showing the reasons for the change.   
* I chose to drop certain genes because they were completely unconnected in the network. With the current framework design, these genes end up with a "network" factor of 0 and would not be chosen during the random sampling (unless all genes in a SNP locus/flank were in this situation - then a uniform probability of picking a gene would be implemented).  

### Load gene info, gene-gene network transition probabilities 
```{r loading1}
## loading gene-gene network matrix 
network_path <- here("supporting_files", "go_propogation_probality_rp_0.3.RData")
## load function reloads datasets that were written with the function "save"
## takes me ~12 seconds to load. matrix has 19080 x 19080 elements
load(network_path)  ## loads pro_p: a dataframe with rownames and colnames

network_genes <- colnames(pro_p)  ## record vector of genes in network
# table(rownames(pro_p)==colnames(pro_p))  ## shows that rownames and colnames are the same
# length(unique(colnames(pro_p)))  ## shows that all gene symbols are unique

network_matrix = pro_p  ## renamed  

## fetch info on all human genes 
gene_info_path <- here("supporting_files", "All_human_genes")
gene_info <- read_delim(gene_info_path, delim = "\t")  # tibble: 53934 rows

## clean unneeded variables
remove(network_path, gene_info_path, pro_p)
```

### EDA showing concerns with gene info's official_name column

First, rows may have the same official_name values but different information (Name, alias_symbol, genomic coordinates). All rows have unique Name column values (Ensembl IDs), but there are some rows with shared official_name values and some rows with shared alias_symbol values.  

I checked rows where official_name=="AGAP4" and "CX3CR1" and found that **rows with the same official_name actually describe different genes. The IDs in the alias_symbol and Name columns are correct and reliable, and the IDs in the official_name column are not reliable.**  
```{r edaGeneSet, eval=FALSE}
skim(gene_info)  ## note that all ENSG IDs (Name col) are unique

## find rows with the same official_name values in the GO network
ex0 <- gene_info %>% 
  group_by(official_name) %>% 
  summarize(count = n()) %>%
  filter(count > 1, official_name %in% network_genes)

## look at these rows with the same official_names
## notice that there are rows where the alias_symbol differs between rows but the official_name doesn't
gene_info %>%
  filter(official_name %in% ex0$official_name) %>%
  select(Name, official_name, alias_symbol, chrom, start_hg19, end_hg19, strand) %>% 
  arrange(official_name)

## If I remove rows where official_name is missing (NA), I lose information on genes in my network. 
gene_info %>% 
  filter(is.na(official_name), alias_symbol %in% network_genes) %>% 
  select(Name, alias_symbol, gene_status, gene_type) %>% 
  arrange(alias_symbol)

remove(ex0)
```

### EDA showing unconnected genes in network  
```{r edaconnect}
## some of the column sums of the matrix are only 0.3 (the probability that the walk stays at the current node), which is odd. The other columns sum ~ 1. 
networkcolsums = colSums(network_matrix)

head(table(networkcolsums))

## some of the columns with this odd column sum
head(which(networkcolsums == 0.3))

## looking into one of the columns 'AAED1': the values relating this node to other nodes is 0
table(network_matrix[,'AAED1'])
network_matrix['AAED1','AAED1']

## look at the row sum for this gene: same issue - the values relating this node to other nodes is 0
sum(network_matrix[15,])
```

### Filtering to get information for genes in network 

Note that the R duplicated() function was not used since it will not return all non-unique rows. It will only return the second/duplicate row.  

The final gene_info dataframe has only connected genes that are in the gene-gene network. There are 17035 rows (unique Ensembl IDs in the 'Name' column) and 17017 unique gene symbols (alias_symbol column). The duplicate gene symbols are from 18 pseudoautosomal genes.   
```{r filteringfull}
unconnected <- names(which(networkcolsums == 0.3))
## get genes that are connected: removes 1296 genes 
network_matrix <- network_matrix %>% filter(!(network_genes %in% unconnected)) %>% select(-unconnected)
rownames(network_matrix) <- colnames(network_matrix)  ## somehow rownames got removed...

network_genes <- colnames(network_matrix)  ## update

gene_info <- gene_info %>% 
  filter(gene_info$alias_symbol %in% network_genes) 
## now have 17064 rows with 17017 unique gene symbols (alias_symbol)
# length(unique(gene_info$alias_symbol)) 

## looking at rows with the same alias_symbol
ex1 <- gene_info %>% 
  group_by(alias_symbol) %>% 
  summarize(count = n()) %>%
  filter(count > 1) %>% 
  arrange(desc(count))
## there are 45 alias_symbol values with 2 rows each  

## get info on all these genes 
ex1 <- gene_info %>% 
  filter(alias_symbol %in% ex1$alias_symbol) %>% 
  arrange(alias_symbol)

## I went through these manually to find the correct rows (ENSG IDs) to keep.  
correct_versions <- c("ENSG00000258315.1",  # C17orf49
                      "ENSG00000258713.1",  # C20orf141: this row has different coordinates from ENSEMBL (ENSEMBL has Chromosome 20: 2,795,633-2,796,479) 
                      "ENSG00000261210.1", "ENSG00000257127.1", # CLEC19A, CLLU1
                      "ENSG00000262246.1", "ENSG00000172115.4", # CORO7, CYCS
                      "ENSG00000197406.5", "ENSG00000091073.13", # DIO3, DTX2
                      "ENSG00000257218.1", "ENSG00000250305.3",  # GATC, KIAA1456
                      "ENSG00000256977.2", # LIMS3: this row has different coordinates from ENSEMBL (ENSEMBL has Chromosome 2: 110,656,005-110,677,180) 
                      "ENSG00000235718.3",  # MFRP: this alias has two legit-looking ENSG IDs, I picked the one with more transcripts/orthologs
                      "ENSG00000259494.1", "ENSG00000148824.12",  # MRPL46, MTG1
                      "ENSG00000100101.12",  # NOL12: this row has different coordinates from ENSEMBL (ENSEMBL has Chromosome 22: 38,077,680-38,170,137) 
                      "ENSG00000257115.1", "ENSG00000262628.1",  # OR11H12, OR1D5
                      "ENSG00000262664.1", "ENSG00000258436.1",  # OVCA2, RNASE12
                      "ENSG00000258818.1", "ENSG00000258366.2",  # RNASE4, RTEL1
                      "ENSG00000020577.9",  # SAMD4A
                      "ENSG00000120341.10",  # SEC16B: this row has different coordinates from ENSEMBL (ENSEMBL has Chromosome 1: 177,893,091-177,953,438) 
                      "ENSG00000261052.1", "ENSG00000006047.8",  # SULT1A3, YBX2
                      "ENSG00000179909.9", "ENSG00000257591.1"  # ZNF154, ZNF625
)

## note that some duplication of alias symbols are legit. Pseudoautosomal regions are matched regions on the X and Y chromosomes, and genes there have two loci/sets of genomic coordinates for the SAME gene symbol.
pseudoautosomal_list <- c("AKAP17A", "ASMT", "ASMTL", "CD99", "CRLF2", "CSF2RA", "DHRSX", "GTPBP6",
                          "IL3RA", "IL9R", "P2RY8", "PLCXD1", "PPP2R3B", "SHOX", "SLC25A6", "SPRY3", 
                          "VAMP7", "ZBED1")  
## source (has more genes than those listed above): https://www.genenames.org/data/genegroup/#!/group/714

## this df has the rows I want to remove from gene_info
ex1 <- ex1 %>% 
  filter(!((Name %in% correct_versions) | alias_symbol %in% pseudoautosomal_list))

gene_info <- gene_info %>% 
  filter(!(gene_info$Name %in% ex1$Name))
# length(unique(gene_info$alias_symbol)) 

remove(ex1, correct_versions, unconnected, networkcolsums)
```

### Get GWAS SNP info
**CHANGES from original code:** removed check on flank_length > 0

```{r getSNPs}
gwas_file_path <- arguments[1]
flank_length <- as.numeric(arguments[2])

## read GWAS SNP file
df_gwas_snps <- read_delim(gwas_file_path, delim = "\t", col_names = TRUE)  ## tibble

## check that needed columns are in GWAS SNP file
## 'all' function = are all values true, aka "are all needed columns in gwas colnames?"
if (!all(c("SNP", "Chr", "Pos_hg19") %in% colnames(df_gwas_snps)))
{
  stop("Wrong column names!\n")
}

## if needed, add prefix "chr" to each chromosome name (in Chr column)
if (substr(df_gwas_snps[1, "Chr"], 1, 3) != "chr")
  ## checks the first 3 characters of the first row
{
  df_gwas_snps$Chr <- paste("chr", df_gwas_snps$Chr, sep = "")
}  ## pastes them together in this order

remove(gwas_file_path)
```

### Find candidate genes around GWAS SNPs      

Because pseudoautosomal genes are on different chromosomes, this flanking method of getting candidate genes will NEVER pick up both copies of a pseudoautosomal gene. **This means that within each candidate gene set for a SNP, there will only be unique alias_symbol values and we can use this column going forward to ID genes.** 
```{r loopFindCandidates}
candidate_genes <- data.frame()  ## initialize. for loop will fill this. 

for(i in 1:nrow(df_gwas_snps))  ## iterate through every SNP 
{  ## upstream of GWAS SNP OR the beginning of chromosome
  flank_start <- max(as.numeric(df_gwas_snps[i,"Pos_hg19"]) - flank_length, 0) 
  ## downstream of GWAS SNP: doesn't cause error if this is beyond length of chromosome
  flank_end <- as.numeric(df_gwas_snps[i,"Pos_hg19"]) + flank_length  
  
  ## grab all genes on same chromosome as SNP, around the flanking region
  temp <- gene_info %>% 
    filter(chrom == as.character(df_gwas_snps[i, "Chr"])) 
  ## True if gene's start is after flank_start and before flank_end.  
  starts_in_flank <- temp$start_hg19 > flank_start & temp$start_hg19 < flank_end   
  ## True if gene's end is after flank_start and before flank_end.  
  ends_in_flank <- temp$end_hg19 > flank_start & temp$end_hg19 < flank_end  
  ## logical OR. This means that genes that partially overlap the flanking region (begin or end in flanking region) are included
  temp <- temp %>% 
    filter(starts_in_flank | ends_in_flank)  
  
  if(nrow(temp) > 0)  ## if there's candidate genes in this locus
  {  ## append matched GWAS SNP info to each candidate gene
    temp <- temp %>% 
      mutate(SNP = as.character(df_gwas_snps[i,"SNP"]), 
             SNP_chr = as.character(df_gwas_snps[i, "Chr"]),
             SNP_pos_hg19 = as.numeric(df_gwas_snps[i, "Pos_hg19"]))
    
    ## adds rows to output 
    candidate_genes <- bind_rows(candidate_genes, temp)  
  }
}

remove(temp, i, starts_in_flank, ends_in_flank, flank_start, flank_end)

## print to screen 
vector_SNPloci <- unique(candidate_genes$SNP)
vector_uniquecandidates <- unique(candidate_genes$alias_symbol)

cat(paste(length(vector_uniquecandidates), "unique ENSG IDs (genes) near", length(vector_SNPloci), "GWAS SNPs were found...\n"))
## note that 1637 rows with 1577 unique ENSG IDs exist: some SNP loci have the same genes. 
# candidate_genes %>% filter(alias_symbol %in% pseudoautosomal_list)  
## There are no pseudoautosomal genes in this set.    

## one SNP out of 108 had no candidate genes found: chr2_146436222_I
# candidate_genes %>% filter(SNP == "chr2_146436222_I")

## original code only filters columns and NOT rows. I filter both since they only work with candidate genes in the end
network_matrix <- network_matrix[vector_uniquecandidates, vector_uniquecandidates] # 1577 x 1577 

## to match ENSG IDs in supporting evidence files, take version number off ENSG IDs 
candidate_genes <- candidate_genes %>% 
  mutate(Name = substr(Name, 1, 15))

remove(vector_uniquecandidates)
```

## Adding supporting evidence 

### Supporting evidence #1: distance from SNP to gene transcription start site (TSS) (DTS)
```{r DTSvar}
candidate_genes <- candidate_genes %>% 
  mutate(DTS = if_else(strand == "+",  # if gene is on strand + 
                       abs(start_hg19 - SNP_pos_hg19),  # True: TSS is at gene start
                       abs(end_hg19 - SNP_pos_hg19)))   # False: TSS is at gene end
```

### Supporting evidence #2: Schizophrenia (SCZ) de novo mutation (DNM) data  
```{r DeNovoMut}
## get DNM data from one file
dnm_path_s3 <- here("supporting_files", "SCZ_DNM", "2014_Feb_De_novo_mutation_in_SCZ_Nature_623_trios_s3.txt")
dnm <- read_delim(dnm_path_s3, delim = "\t")  
## note that the child phenotypes are ASD, control, ID, SZ, Unaffected sibling
dnm <- dnm %>% 
  filter(`Child phenotype` == "SZ")  ## get DNM in individuals with schizophrenia  

## get DNM from the other file
dnm_path_s2 <- here("supporting_files", "SCZ_DNM", "2014_Feb_De_novo_mutation_in_SCZ_Nature_623_trios_s2.txt")
fromer <- read_delim(dnm_path_s2, delim = "\t") 
fromer <- fromer %>% mutate(`Child phenotype` = "SZ", Study = "Fromer")
## all of these are DNM in individuals with schizophrenia   

## merge these dataframes  
dnm <- full_join(dnm, fromer)  
remove(fromer, dnm_path_s2, dnm_path_s3)

## There are 10 types of gene annotations. Authors kept 6: "esplice", "frameshift", "nonsense", "missense", "codon-deletion", "code-insertion". More efficient to take out the others
dnm <- dnm %>% 
  filter((`Gene annotations` != "silent") & (`Gene annotations` != "silent,missense") &
           (`Gene annotations` != "silent,silent") & (`Gene annotations` != "start-lost")) 
```

### Supporting evidence #3: SCZ-specific differential expression (DE) data  
This data is already in p-value form  
```{r DE}
de_path <- here("supporting_files", "SCZ_DE", "CMC_MSSM-Penn-Pitt_DLPFC_mRNA_IlluminaHiSeq2500_gene-adjustedSVA-differentialExpression-includeAncestry-DxSCZ-DE.tsv")
dexpr <- read_delim(de_path, delim = "\t")

remove(de_path)
```

### Supporting evidence #4-5: brain Hi-C data (CP + GZ)
```{r brainHiC}
cp_path <- here("supporting_files", "BrainHiC", "S22_TSS_CP.txt")
gz_path <- here("supporting_files", "BrainHiC", "S23_TSS_GZ.txt")
cp <- read_delim(cp_path, delim = "\t") %>% select(-X8)  ## cp has 221069 rows
gz <- read_delim(gz_path, delim = "\t")  ## gz has 228323 rows

## make a column of enhancer coordinate info that makes it easy to compare positions  
cp <- cp %>% 
  unite(col = "enhancer", c("chr", "interacting_bin_start", "interacting_bin_end"), sep = ":")  
gz <- gz %>% 
  unite(col = "enhancer", c("chr", "interacting_bin_start", "interacting_bin_end"), sep = ":")  

## count how many unique enhancers (by chr, coordinates) per gene in dataset
cp_counts <- cp %>% 
  group_by(ENSGID_for_TSS) %>% 
  summarize(cp_enhancer_no = n_distinct(enhancer))  ## 34255 ENSG IDs/rows
gz_counts <- gz %>% 
  group_by(ENSGID_for_TSS) %>% 
  summarize(gz_enhancer_no = n_distinct(enhancer))  ## 35019 ENSG IDs/rows

## adds columns to candidate_gene df, replace NAs with 0.  
candidate_genes <- left_join(candidate_genes, cp_counts, by = c("Name" = "ENSGID_for_TSS"))
candidate_genes <- left_join(candidate_genes, gz_counts, by = c("Name" = "ENSGID_for_TSS"))
candidate_genes <- candidate_genes %>% 
  mutate(cp_enhancer_no = replace_na(cp_enhancer_no, 0), 
         gz_enhancer_no = replace_na(gz_enhancer_no, 0))

remove(cp_path, gz_path, cp_counts, gz_counts)
```

### Supporting evidence #6: capture Hi-C data (cell line, not specific to brain)  
```{r GM12878HiC}
capHiC_path <- here("supporting_files", "capHiC", "GM12878_DRE_number")
capHiC <- read_delim(capHiC_path, delim = "\t") 

## adds cap4_enhancer_no column to table
candidate_genes <- left_join(candidate_genes, 
                             capHiC %>% select(Name, cap4_enhancer_no), 
                             by = c("Name"))

remove(capHiC_path)
```

### Supporting evidence #7: FANTOM5 data  
I get 66899 enhancer-promoter links after filtering for FDR < 1 (mentioned in iRIGS paper's methods)!   
```{r addingFANTOM5}
FANTOM_path <- here("supporting_files", "Fantom5", "enhancer_tss_associations.bed.txt")
FANTOM5_data <- read_delim(FANTOM_path, delim = "\t")  ## 66942 rows
 
## parsing name column: adds NA to 35 rows
FANTOM5_data <- FANTOM5_data %>% 
  separate(col = name, into = c("pos_range", "NCBI_ID", "gene", "Rstat", "FDR"), sep = ";") 
## removes those 35 rows. have 66907 left
FANTOM5_data <- drop_na(FANTOM5_data)  
## get numeric FDR values as a column
FANTOM5_data <- FANTOM5_data %>%  
  separate(col = FDR, into = c("label", "fdr"), sep = ":", convert = TRUE) %>% 
  select(-label)

## select all genes with FDR < 1 
FANTOM5_data <- FANTOM5_data %>% 
  filter(fdr < 1)  ## removes 8 rows: left with 66899 rows

## count how many unique enhancers (identified by position) per gene in dataset
FANTOM5_enhancer_count <- FANTOM5_data %>% 
  group_by(gene) %>% 
  summarize(fantom5_enhancer_no = n_distinct(pos_range))   ## 11602 rows (unique gene symbols)

## adds fantom5_enhancer_no column to table
candidate_genes <- left_join(candidate_genes, 
                             FANTOM5_enhancer_count, 
                             by = c("alias_symbol" = "gene"))
# table(is.na(candidate_genes$fantom5_enhancer_no))
## 597 NAs were added so need to replace those with 0. 
candidate_genes <- candidate_genes %>% 
  mutate(fantom5_enhancer_no = replace_na(fantom5_enhancer_no, 0)) 

remove(FANTOM_path, FANTOM5_enhancer_count)
```

## Calculating and combining p-values

**CHANGES from original code:**

* replaced missing values in capHiC column with 0, to match how missing values were handled in other enhancer columns    
* kept surrogate Bayes factor for each gene-SNP pair (used SNP ID and alias_symbol to retrieve during Gibbs sampling)  
 
{?? why did original code appear to keep only one "extra_weight" (the first match) per unique gene/"official_name" value?}  
{?? The Mahalanobis transformation is supposed to create variables that follow the univariable Gaussian (normal) distribution but I feel like the transformed variables don't...}  
{?? Why is dnm probability calculated using a hard-coded number: 20,000?}  
{?? why get negative log of the final "extra_weights"? To get bigger numbers when meta p-value is small?}   

### Mahalanobis transformation (ZCA whitening)   

* Done on DTS, enhancer columns of supporting data (capHiC, FANTOM5, brain-specific HiC)  
* This creates uncorrelated variables, which can then be used to find p-values  
```{r getMahalCols}  
## process data to get mean-centered values  
zca_cols <- c('DTS', 'cap4_enhancer_no', 'fantom5_enhancer_no', 'cp_enhancer_no', 'gz_enhancer_no')

extra_evidence <- candidate_genes %>% select(zca_cols) 

summary(extra_evidence)
```

Notice that only one column has NA values: cap4_enhancer_no. This is because DTS is calculated from genomic coordinates and the other enhancer columns replaced their missing values with 0 (in the original code, the default value for the columns is 0: see extra_evi.R.  

The original code replaces the missing values here, aka cap4_enhancer_no's missing values, with the median for the column. I want to treat this column the same as the other enhancer columns, so I replace the 3 missing values with 0.  
```{r MahalNAfill}
extra_evidence <- extra_evidence %>% 
  mutate(cap4_enhancer_no = replace_na(cap4_enhancer_no, 0))
```

Next is calculating the Mahalanobis transformation.  
```{r Mahalanobis}
## first calculate covariance matrix, and SVD on the covariance matrix
cov_extra <- cov(extra_evidence)  ## gives 5 x 5 list of lists (matrix)
svd_extra <- svd(cov_extra)  ## calculates d (list of 5 values), u (5 x 5 matrix), v (5 x 5 matrix) 

## find 1 / sqrt () for each value in d (same as () ^ (-0.5)). Then make these values into a diagonal matrix = new_s_diag (values only on diagonal, 0 otherwise). 
new_s_diag <- diag( svd_extra$d ^ (-0.5) )
## make new set of transformations: U * new_s_diag * V(transpose). Looks like "making a new covariance matrix for a new set of variables". 
svd_newtrans <- svd_extra$u %*% new_s_diag %*% t(svd_extra$v)  
## %*% is matrix multiplication, on 5 x 5 matrices
```

The Mahalanobis transformation is done on mean-centered values.    
```{r transform}
## get mean-centered values 
extra_trans <- extra_evidence %>% 
  mutate_all(function(x) x - mean(x))
extra_trans <- t(extra_trans)  ## use class() function: this is a matrix

## transforming mean-centered values  
extra_trans <- t(svd_newtrans %*% extra_trans) ## 5 x 5 matrix * 5 x 1732 matrix = 5 x 1732 matrix. 
## Then transpose to get it back in the same form as original df: 1732 genes x 5 variables. 

## get extra_trans to be a dataframe again. 
colnames(extra_trans) <- colnames(extra_evidence)  
extra_trans <- as.data.frame(extra_trans)  ## df has same row/gene-SNP pairs order as candidate_genes
```

With the current setup, we would need to calculate the p-value for the lower tail for the DTS variable and the p-value for the upper tail for the other variables. This is because...  

* DTS: the smaller the number (distance to SNP < expected), the more likely it is to be a disease gene (higher weight)  
* for the others: the larger the number (number of DRE-promoter links > expected), the more likely it is to be a disease gene (higher weight). Recall the assumption made (Description of iRIGS above).    

For the other columns, we flip their signs to flip the tails. Now for all variables, the smaller the number, the more likely it is to be a disease gene.    
```{r switchsign}
larger_likely_disease <-  c('cap4_enhancer_no', 'fantom5_enhancer_no', 
                            'cp_enhancer_no', 'gz_enhancer_no')

extra_trans <- extra_trans %>% 
  mutate_at(larger_likely_disease, function(x) -x)
```

Make p-values
```{r makep}
## supposedly all variables now follow univariate standard Gaussian dist, but I don't feel like the plots support this...
# skim(extra_trans)
# qqnorm(extra_trans$DTS)

## run pnorm to get p-values/probabilities for each column
## new df has same row/gene-SNP pairs order as candidate_genes
extra_p <- extra_trans %>% 
  mutate_all(pnorm)  ## default for function gets lower tail

remove(extra_evidence, cov_extra, svd_extra, extra_trans, new_s_diag, svd_newtrans, larger_likely_disease, zca_cols)
```

### Add p-values for DNM and DE data 

Add and process p-values from DNM (de-novo mutation) and DE (differential expression) data. This data is schizophrenia specific.  
Missing DE p-values are replaced with median (not replaced with 0 because that would make them significant...).   
```{r add_DNM_DE_pvalues}
## DNM data 
dnm_prob <- length(unique(dnm$Genes)) / 20000  

## find only 59 candidate genes that had schizophrenia-associated DNM
# table(candidate_genes$alias_symbol %in% dnm$Genes)  
mask <- candidate_genes$alias_symbol %in% dnm$Genes
extra_p <- extra_p %>% 
  mutate(dnm = if_else(mask, 
                       dnm_prob,  # genes with dnm get this p-value
                       1 - dnm_prob))  # genes w/o dnm get other p-value

## DE data 
## note that extra_p df has same row order as candidate_genes
## add column of ENSG IDs to make merge possible with dexpr
extra_p <- extra_p %>% mutate(Name = candidate_genes$Name)

extra_p <- left_join(extra_p, 
                     dexpr %>% select(genes, P.Value), 
                     by = c("Name" = "genes"))

extra_p <- extra_p %>% select(-Name)  ## remove Name column: used only for merge
extra_p <- extra_p %>% rename(DE = P.Value)  ## rename column
## 1309 genes have DE info, 423 have missing values 

extra_p <- extra_p %>%   ## replace NAs with median for DE column
  mutate(DE = replace_na(DE, median(extra_p$DE, na.rm = TRUE)))

remove(dnm_prob, mask)
```

### Fisher's product method of combining data  
See: https://en.wikipedia.org/wiki/Fisher%27s_method, https://stats.stackexchange.com/questions/59360/fishers-method-for-combing-p-values-what-about-the-lower-tail  

When individual p-values are small, the meta-test-statistic will be large. When all individual null hypotheses are true and their test stats/p-values are independent, this meta-test-statistic has a chi-squared dist with 2k degrees of freedom (k = number of tests that are combined).  

The meta p-values are then calculated using what the paper's supplement (pgs. 2-3) describes as the "inverse cumulative distribution function (CDF) of chi-squared (chi2) distribution." In practice, this is 1 - (chi2's CDF). In R, this is found using the CDF function (pchisq) with lower.tail = FALSE.  

**The final values here are used as surrogates for the Bayesian factor!** This is explained in the Supplementary Note for the iRIGS paper.      

```{r fishersproduct}
## From Fisher's method: chi-squared dist has degrees of freedom = 2 * number of tests  
fishers_chi_df <- 2 * ncol(extra_p) 

## R default log IS A NATURAL LOG 
## this is the meta-test-statistic. sum is across the row 
df_transFisher <- extra_p %>% 
  transmute(transformed_Fisherprod = -2 * rowSums(log(extra_p)))

## get p-values of the meta-test-statistic:  
df_transFisher <- df_transFisher %>% 
  mutate(transformed_Fisherprod = pchisq(transformed_Fisherprod, 
                                  df = fishers_chi_df, 
                                  lower.tail = FALSE))

## get negative log of all these weights. I think this is so larger weights -> the gene is more likely to be important for the SNP GWAS signal. 
df_transFisher <- df_transFisher %>% 
  mutate(transformed_Fisherprod = -log(transformed_Fisherprod))

## assign as column in candidate_genes
candidate_genes <- bind_cols(candidate_genes, df_transFisher)

remove(fishers_chi_df)
```

I can explore what's going on with Fisher's method (checking calculations with the metap::sumlog function as well):  
```{r pcheck}
# ## checking calculations
# metap::sumlog(extra_p[2,])

## what had a very high surrogate Bayes factor? = a likely risk gene 
which(df_transFisher == max(df_transFisher))
candidate_genes[1494,]  ## there were a lot of enhancers found and a smaller distance, surrogate Bayes factor is ~ 66.9
extra_p[1494,]  ## one of the individual p-values is very low
metap::sumlog(extra_p[1494,])  ## test stat is large

## what had a very low surrogate Bayes factor? = not likely risk gene
which(df_transFisher == min(df_transFisher))
candidate_genes[1038,]  ## almost no enhancers found, distance from SNP is far, surrogate Bayes factor is ~ 0.0007 
extra_p[1038,]  ## all individual p-values are high
metap::sumlog(extra_p[1038,])  ## test stat is small

## another with a very high surrogate Bayes factor
which(df_transFisher >= 50)
candidate_genes[127,]  ## close distance, most enhancer numbers are high, surrogate Bayes factor ~ 52.6
extra_p[127,]  ## two p-values are very small  
metap::sumlog(extra_p[127,])  ## test stat is high
```

## Gibbs sampling: burn-in step
CHANGES from original code:   
* Gibbs sampling is not done on SNP loci with only one candidate gene. This is a change from the original code, which would add a sample identical to the previous sample.  

Takes me ~ 25 min to run, ~ 606 runs thru all loci with > 1 candidate gene. 
```{r burn-in1, results='hide'}
t0 <- proc.time()  ## used to record time in system

## round = a set of sampling thru all loci
max_burnin_rounds <- 3000  ### maximum sampling round allowed at burn in step
flag_exclude_sharedrowcols <- T  ## flag to remove candidate gene (row) info from network/col info 

## square-root of the threshold we want! stopping criterion
stopping_threshold <- sqrt(10 ^ (-4) )  
dif <- stopping_threshold + 1  ## used for stopping criterion

burnin_rounds <- 0  ## counter for number of rounds thru all SNP loci
## used to record counts of SNP-gene pairs from all previous rounds
burnin_gene_counts <- candidate_genes %>% 
  select(SNP, alias_symbol) %>% 
  mutate(counts = 0)  

## =set starting set: 1 per SNP locus 
set.seed(431)  ## for now, set seed to watch stuff
## note that sample_n works only for local tables
current_geneset <- candidate_genes %>% 
  group_by(SNP) %>% 
  sample_n(1) %>% 
  select(SNP, alias_symbol)
vector_SNPloci <- current_geneset$SNP  ## so I can iterate in the same order as current_gene_set's rows

## get SNP loci with only one candidate gene: no need to use Gibbs sampling to find the most probable candidate
SNPs_with_one_candidate <- candidate_genes %>% 
  group_by(SNP) %>% 
  summarize(count = n()) %>% 
  filter(count == 1)

## how many SNP loci are sampled per round
SNPs_per_round <- length(vector_SNPloci) - nrow(SNPs_with_one_candidate)

## for debugging code inside the for loop
# i <- vector_SNPloci[1]

## outer loop: checking dif in calculated prob from end of last round and checking number of rounds 
while (dif > stopping_threshold & burnin_rounds < max_burnin_rounds)
{
  ## store Gibbs sample set for the current round
  burnin_gibbsets <- tibble()  
  
  for (i in vector_SNPloci)  ## inner loop: for each SNP loci...
  {  ## note that if there is only one candidate gene for the SNP loci, the chunk doesn't run and the loci is effectively SKIPPED for Gibbs sampling
    if (!(i %in% SNPs_with_one_candidate$SNP))
    { 
      ## grab Bayesian factor surrogate for each candidate gene in the SNP locus i 
      currentloci_genes <- candidate_genes %>% 
        filter(SNP == i) %>% 
        select(alias_symbol, transformed_Fisherprod)
      
      ## look at the other loci (not i)   
      otherloci_genes <- current_geneset %>% 
        filter(SNP != i) 
      if (flag_exclude_sharedrowcols == TRUE)  ## take out genes that are also in SNP locus i 
      {  
        otherloci_genes <- otherloci_genes %>% 
          filter(!(alias_symbol %in% currentloci_genes$alias_symbol))
        if (nrow(otherloci_genes) == 0)  
          ## if this takes out all of the other genes, this would cause issues downstream. I haven't had this flag go off before. Would probably have to be dealt with by drawing a new current_geneset similar to initialization.  
        {
          stop(paste("Error: too much overlap between the gene set", current_geneset$alias_symbol, 
                     "and the SNP locus", i, "\nled to issue calculating network factor\n"))
        }
      }
      ## retrieve network info: rows are candidate genes from SNP locus i, columns are genes from other SNP loci (not-i)
      networkinfo <- network_matrix[currentloci_genes$alias_symbol, otherloci_genes$alias_symbol]
      ## row sums (add together prob from other SNP loci)
      network_factors <- rowSums(networkinfo) %>%  
        as.data.frame() %>%         ## get into a format to join with currentloci_genes df
        rownames_to_column(var = "alias_symbol") %>% 
        rename(network_factor = ".")
      
      ## calculate probability weighting of candidate genes (distribution) using network factor and surrogate Bayesian factor     
      currentloci_genes <- left_join(currentloci_genes, network_factors, by = "alias_symbol")
      currentloci_genes <- currentloci_genes %>% 
        mutate(probweights = transformed_Fisherprod * network_factor)
      
      ## if all the calculated weights are zero, set uniform prob dist 
      if (sum(currentloci_genes['probweights']) == 0)
      {
        currentloci_genes <- currentloci_genes %>% 
          mutate(probweights = 1)
      }
      
      ## choose loci's new state using the probability dist 
      newstate <- sample(currentloci_genes$alias_symbol, 
                         size = 1, 
                         prob = currentloci_genes$probweights)
      ## find row for SNP locus i, replace the entry with the new state
      current_geneset[current_geneset$SNP==i, "alias_symbol"] <- newstate
      
      ## record the gene set
      gibbs_sample <- spread(current_geneset, key = SNP, value = alias_symbol)
      burnin_gibbsets <- bind_rows(burnin_gibbsets, gibbs_sample)  ## will match by column name (SNP ID)
      
      remove(currentloci_genes, otherloci_genes, network_factors, networkinfo, gibbs_sample)
    }  ## end the if statement for only Gibbs sampling on SNP loci with > 1 candidate gene 
  }  ## end for loop of iterating thru each SNP loci
  
  ## a round is finished so increment this for stopping criteria 
  burnin_rounds <- burnin_rounds + 1
  if (burnin_rounds %% 50 == 0)
  {
    cat("Burn-in sampling, finished full round thru SNP loci:", burnin_rounds, "\n")
  }
  
  ## calculate dif for stopping criteria 
  ## first, get gene counts for each SNP loci from the set of Gibbs samples from this round
  new_counts <- data.frame()  
  for (i in vector_SNPloci)
  {
    temp <- as.data.frame(burnin_gibbsets[i] %>% table(), stringsAsFactors = FALSE) %>% 
      rename(alias_symbol = ".", new_counts = Freq) %>% 
      mutate(SNP = i)
    new_counts <- bind_rows(new_counts, temp)
  }
  remove(temp)
  
  ## merge new_counts with main counts df, replace NAs with 0 in new_count column
  burnin_gene_counts <- left_join(burnin_gene_counts, new_counts, by = c("SNP", "alias_symbol"))
  burnin_gene_counts <- burnin_gene_counts %>% 
    mutate(new_counts = replace_na(new_counts, 0))
  
  ## add together the old counts and new_counts -> get new sampling counts
  burnin_gene_counts <- burnin_gene_counts %>% 
    mutate(new_counts = new_counts + counts)
  
  ## starting with the second round, calculate dif between sampling probabilities from previous round
  if (burnin_rounds > 1)
  {
    burnin_gene_counts <- burnin_gene_counts %>% 
      mutate(freq_old = counts / ((burnin_rounds - 1) * SNPs_per_round), 
             freq_new = new_counts / (burnin_rounds * SNPs_per_round))
    ## sum of squared differences, then square-root. I think the reasoning is:
    ## (1) sign/direction of difference won't matter and (2) we avoid dealing with very small numbers (loss of precision)
    dif <- sqrt( sum( (burnin_gene_counts$freq_new - burnin_gene_counts$freq_old) ^ 2 ) ) 
    cat("dif is", dif, "\n")
  }
  
  ## replace old counts with new_counts. 
  burnin_gene_counts <- burnin_gene_counts %>% 
    mutate(counts = new_counts) %>% 
    select(SNP, alias_symbol, counts)
  
  remove(burnin_gibbsets, new_counts)
}

print(proc.time()-t0) 
cat(paste("number of burn-in rounds of Gibbs sampling: ", burnin_rounds))
```

## Gibbs sampling: finding sampling probability step
After burn-in.
Takes me ~26 min, 619 rounds. 
```{r postburn, results='hide'}
## keep current_geneset from last burn-in round, vector_SNPloci, SNPs_with_one_candidate, SNPs_per_round

t0 <- proc.time()  ## used to record time in system

## round = a set of sampling thru all loci
max_postburn_rounds <- 3000    ### maximum sampling round allowed at post burn in step

## reset variables
dif <- stopping_threshold + 1  ## used for stopping criterion
postburn_rounds <- 0  ## counter for number of rounds thru all SNP loci
## used to record counts of SNP-gene pairs from all previous rounds
postburn_gene_counts <- candidate_genes %>% 
  select(SNP, alias_symbol) %>% 
  mutate(counts = 0)  

## outer loop: checking dif in calculated prob from end of last round and checking number of rounds 
while (dif > stopping_threshold & postburn_rounds < max_postburn_rounds)
{
  ## store Gibbs sample set for the current round
  postburn_gibbsets <- tibble()  
  
  for (i in vector_SNPloci)  ## inner loop: for each SNP loci...
  {  ## note that if there is only one candidate gene for the SNP loci, the chunk doesn't run and the loci is effectively SKIPPED for Gibbs sampling
    if (!(i %in% SNPs_with_one_candidate$SNP))
    { 
      ## grab Bayesian factor surrogate for each candidate gene in the SNP locus i 
      currentloci_genes <- candidate_genes %>% 
        filter(SNP == i) %>% 
        select(alias_symbol, transformed_Fisherprod)
      
      ## look at the other loci (not i)   
      otherloci_genes <- current_geneset %>% 
        filter(SNP != i) 
      if (flag_exclude_sharedrowcols == TRUE)  ## take out genes that are also in SNP locus i 
      {  
        otherloci_genes <- otherloci_genes %>% 
          filter(!(alias_symbol %in% currentloci_genes$alias_symbol))
        if (nrow(otherloci_genes) == 0)  
          ## if this takes out all of the other genes, this would cause issues downstream. I haven't had this flag go off before. Would probably have to be dealt with by drawing a new current_geneset similar to initialization.  
        {
          stop(paste("Error: too much overlap between the gene set", current_geneset$alias_symbol, 
                     "and the SNP locus", i, "\nled to issue calculating network factor\n"))
        }
      }
      ## retrieve network info: rows are candidate genes from SNP locus i, columns are genes from other SNP loci (not-i)
      networkinfo <- network_matrix[currentloci_genes$alias_symbol, otherloci_genes$alias_symbol]
      ## row sums (add together prob from other SNP loci)
      network_factors <- rowSums(networkinfo) %>%  
        as.data.frame() %>%         ## get into a format to join with currentloci_genes df
        rownames_to_column(var = "alias_symbol") %>% 
        rename(network_factor = ".")
      
      ## calculate probability weighting of candidate genes (distribution) using network factor and surrogate Bayesian factor     
      currentloci_genes <- left_join(currentloci_genes, network_factors, by = "alias_symbol")
      currentloci_genes <- currentloci_genes %>% 
        mutate(probweights = transformed_Fisherprod * network_factor)
      
      ## if all the calculated weights are zero, set uniform prob dist 
      if (sum(currentloci_genes$probweights) == 0)
      {
        currentloci_genes <- currentloci_genes %>% 
          mutate(probweights = 1)
      }
      
      ## choose loci's new state using the probability dist 
      newstate <- sample(currentloci_genes$alias_symbol, 
                         size = 1, 
                         prob = currentloci_genes$probweights)
      ## find row for SNP locus i, replace the entry with the new state
      current_geneset[current_geneset$SNP==i, "alias_symbol"] <- newstate
      
      ## record the gene set
      gibbs_sample <- spread(current_geneset, key = SNP, value = alias_symbol)
      postburn_gibbsets <- bind_rows(postburn_gibbsets, gibbs_sample)  ## will match by column name (SNP ID)
      
      remove(currentloci_genes, otherloci_genes, network_factors, networkinfo, gibbs_sample)
    }  ## end the if statement for only Gibbs sampling on SNP loci with > 1 candidate gene 
  }  ## end for loop of iterating thru each SNP loci
  
  ## a round is finished so increment this for stopping criteria 
  postburn_rounds <- postburn_rounds + 1
  if (postburn_rounds %% 50 == 0)
  {
    cat("Burn-in sampling, finished full round thru SNP loci:", postburn_rounds, "\n")
  }
  
  ## calculate dif for stopping criteria 
  ## first, get gene counts for each SNP loci from the set of Gibbs samples from this round
  new_counts <- data.frame()  
  for (i in vector_SNPloci)
  {
    temp <- as.data.frame(postburn_gibbsets[i] %>% table(), stringsAsFactors = FALSE) %>% 
      rename(alias_symbol = ".", new_counts = Freq) %>% 
      mutate(SNP = i)
    new_counts <- bind_rows(new_counts, temp)
  }
  
  ## merge new_counts with main counts df, replace NAs with 0 in new_count column
  postburn_gene_counts <- left_join(postburn_gene_counts, new_counts, by = c("SNP", "alias_symbol"))
  postburn_gene_counts <- postburn_gene_counts %>% 
    mutate(new_counts = replace_na(new_counts, 0))
  
  ## add together the old counts and new_counts -> get new sampling counts
  postburn_gene_counts <- postburn_gene_counts %>% 
    mutate(new_counts = new_counts + counts)
  
  ## starting with the second round, calculate dif between sampling probabilities from previous round
  if (postburn_rounds > 1)
  {
    postburn_gene_counts <- postburn_gene_counts %>% 
      mutate(freq_old = counts / ((postburn_rounds - 1) * SNPs_per_round), 
             freq_new = new_counts / (postburn_rounds * SNPs_per_round))
    ## sum of squared differences, then square-root. I think the reasoning is:
    ## (1) sign/direction of difference won't matter and (2) we avoid dealing with very small numbers (loss of precision)
    dif <- sqrt( sum( (postburn_gene_counts$freq_new - postburn_gene_counts$freq_old) ^ 2 ) ) 
    cat("dif is", dif, "\n")
  }
  
  ## replace old counts with new_counts. 
  postburn_gene_counts <- postburn_gene_counts %>% 
    mutate(counts = new_counts) %>% 
    select(SNP, alias_symbol, counts)
  
  remove(postburn_gibbsets, new_counts)
}

print(proc.time()-t0)
cat(paste("number of post-burn-in rounds of Gibbs sampling: ", postburn_rounds))
```

## Output
Will write files and display the final dataframes.    
```{r output1}
cat("Processing and recording the final sampling probabilities!\n")

## calculate final sampling probability(num_rounds * SNPs_per_round)
postburn_gene_counts <- postburn_gene_counts %>% 
  mutate(posterior_probability = counts / (postburn_rounds * SNPs_per_round))

## final dataframe
postburn_gene_counts <- postburn_gene_counts %>% 
  select(SNP, alias_symbol, posterior_probability) %>%
  rename(candidate_gene_symbol = alias_symbol) %>% 
  arrange(SNP, desc(posterior_probability))

postburn_gene_counts

## create directory for result files
results_dir_path <- arguments[3] 
if (!file.exists(results_dir_path)) 
{ dir.create(results_dir_path,showWarnings=T,recursive=T) }

## create original result file
result1_path <- arguments[4] 
write_delim(postburn_gene_counts, result1_path, delim = "\t")

## Build table like in Supplementary Table 1 of paper
## count number of candidate genes per SNP locus
supple1 <- postburn_gene_counts %>% 
  group_by(SNP) %>% 
  top_n(1, posterior_probability)

SNPs_per_loci <- postburn_gene_counts %>% 
  group_by(SNP) %>% 
  summarize(number_of_candidates = n())

supple1 <- left_join(supple1, SNPs_per_loci, by = "SNP")
## reorder to match supplementary table 
supple1 <- supple1 %>% 
  select(candidate_gene_symbol, posterior_probability, SNP, number_of_candidates)

## create supplementary table 1 result file
result2_path <- arguments[5] 
write_delim(supple1, result2_path, delim = "\t")
```